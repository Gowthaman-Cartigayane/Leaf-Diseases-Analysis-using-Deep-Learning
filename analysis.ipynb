{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83d82cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 10\n",
      "['Tomato_Bacterial_spot', 'Tomato_Early_blight', 'Tomato_healthy', 'Tomato_Late_blight', 'Tomato_Leaf_Mold', 'Tomato_Septoria_leaf_spot', 'Tomato_Spider_mites_Two_spotted_spider_mite', 'Tomato__Target_Spot', 'Tomato__Tomato_mosaic_virus', 'Tomato__Tomato_YellowLeaf__Curl_Virus']\n",
      "Number of classes: 10\n",
      "['Tomato_Bacterial_spot', 'Tomato_Early_blight', 'Tomato_healthy', 'Tomato_Late_blight', 'Tomato_Leaf_Mold', 'Tomato_Septoria_leaf_spot', 'Tomato_Spider_mites_Two_spotted_spider_mite', 'Tomato__Target_Spot', 'Tomato__Tomato_mosaic_virus', 'Tomato__Tomato_YellowLeaf__Curl_Virus']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_dir = r\"plant_disease_dataset_10_classes_split/plant_disease_dataset_10_classes_split/train\"\n",
    "\n",
    "test_dir = r\"plant_disease_dataset_10_classes_split/plant_disease_dataset_10_classes_split/test\"\n",
    "\n",
    "train_classes = os.listdir(train_dir)\n",
    "test_classes = os.listdir(test_dir)\n",
    "print(f\"Number of classes: {len(train_classes)}\")\n",
    "print(train_classes)\n",
    "\n",
    "print(f\"Number of classes: {len(test_classes)}\")\n",
    "print(test_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc718658",
   "metadata": {},
   "source": [
    "Transformation of Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f5d2dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [transforms.Resize((224, 224)),\n",
    "     transforms.ToTensor(),\n",
    "transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])])\n",
    "\n",
    "train_dataset = ImageFolder(train_dir, transform = train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b6371a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sample in train_dataset: 12006\n",
      "=========================\n",
      "Tomato_Bacterial_spot -> 1595\n",
      "Tomato_Early_blight -> 750\n",
      "Tomato_Late_blight -> 1431\n",
      "Tomato_Leaf_Mold -> 714\n",
      "Tomato_Septoria_leaf_spot -> 1328\n",
      "Tomato_Spider_mites_Two_spotted_spider_mite -> 1257\n",
      "Tomato__Target_Spot -> 1053\n",
      "Tomato__Tomato_YellowLeaf__Curl_Virus -> 2406\n",
      "Tomato__Tomato_mosaic_virus -> 279\n",
      "Tomato_healthy -> 1193\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "train_labels = train_dataset.targets\n",
    "\n",
    "count_per_classes = Counter(train_labels)\n",
    "\n",
    "print(f\"Total Sample in train_dataset:\", len(train_dataset))\n",
    "print(\"=\"*25)\n",
    "\n",
    "for i, class_name in enumerate(train_dataset.classes):\n",
    "    print(f\"{class_name:<10} -> {count_per_classes[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00d9229",
   "metadata": {},
   "source": [
    "Transformation of Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b2a1286",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose(\n",
    "    [transforms.Resize((224,224)),\n",
    "     transforms.ToTensor(),\n",
    "transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_dataset = ImageFolder(test_dir, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e200f228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total Sample in the test dataset: 4005\n",
      "=========================\n",
      "Tomato_Bacterial_spot -> 532\n",
      "Tomato_Early_blight -> 250\n",
      "Tomato_Late_blight -> 478\n",
      "Tomato_Leaf_Mold -> 238\n",
      "Tomato_Septoria_leaf_spot -> 443\n",
      "Tomato_Spider_mites_Two_spotted_spider_mite -> 419\n",
      "Tomato__Target_Spot -> 351\n",
      "Tomato__Tomato_YellowLeaf__Curl_Virus -> 802\n",
      "Tomato__Tomato_mosaic_virus -> 94\n",
      "Tomato_healthy -> 398\n"
     ]
    }
   ],
   "source": [
    "test_labels = test_dataset.targets\n",
    "count_per_classes = Counter(test_labels)\n",
    "\n",
    "print(f\"The total Sample in the test dataset:\", len(test_dataset))\n",
    "print(\"=\"*25)\n",
    "\n",
    "for i, class_name in enumerate(test_dataset.classes):\n",
    "    print(f\"{class_name:<10} -> {count_per_classes[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532c49fa",
   "metadata": {},
   "source": [
    "Splitting the train data into Train and Val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2f4fd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample in the train dataset: 12006\n",
      "The size of train dataset: 8404\n",
      "The size of val dataset: 3602\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "torch.manual_seed = 32\n",
    "\n",
    "total_size = len(train_dataset)\n",
    "\n",
    "train_size = int(0.70*len(train_dataset))\n",
    "val_size = total_size - train_size\n",
    "\n",
    "train_ds, val_ds = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "print(\"The sample in the train dataset:\", len(train_dataset))\n",
    "print(\"The size of train dataset:\", train_size)\n",
    "print(\"The size of val dataset:\", val_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b039f81",
   "metadata": {},
   "source": [
    "Using DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ea7d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5987859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)                  \n",
    "        loss = F.cross_entropy(out, labels)  \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)                    \n",
    "        loss = F.cross_entropy(out, labels)   \n",
    "        acc = accuracy(out, labels)           \n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc.detach()}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   \n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      \n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result, is_best=False):\n",
    "        msg = (\n",
    "            f\"Epoch {epoch} | \"\n",
    "            f\"Train Acc: {result['train_acc']:.4f} | \"\n",
    "            f\"Train Loss: {result['train_loss']:.4f} | \"\n",
    "            f\"Val Loss: {result['val_loss']:.4f} | \"\n",
    "            f\"Val Acc: {result['val_acc']:.4f}\"\n",
    "        )\n",
    "        if is_best:\n",
    "            msg += \"  <-- Saving best model \"\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ca66000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class ResNet(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = models.resnet50(pretrained=True)\n",
    "\n",
    "        \n",
    "        num_ftrs = self.network.fc.in_features\n",
    "\n",
    "       \n",
    "        self.network.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),               \n",
    "            nn.Linear(num_ftrs, len(train_dataset.classes))\n",
    "        )\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc94478f",
   "metadata": {},
   "source": [
    "Making changes in the Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fcac28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(ImageClassificationBase):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.network = models.resnet50(weights= None)\n",
    "        \n",
    "        num_ftrs = self.network.fc.in_features\n",
    "\n",
    "        self.network.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4), \n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b828635e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"network.conv1.weight\", \"network.bn1.weight\", \"network.bn1.bias\", \"network.bn1.running_mean\", \"network.bn1.running_var\", \"network.layer1.0.conv1.weight\", \"network.layer1.0.bn1.weight\", \"network.layer1.0.bn1.bias\", \"network.layer1.0.bn1.running_mean\", \"network.layer1.0.bn1.running_var\", \"network.layer1.0.conv2.weight\", \"network.layer1.0.bn2.weight\", \"network.layer1.0.bn2.bias\", \"network.layer1.0.bn2.running_mean\", \"network.layer1.0.bn2.running_var\", \"network.layer1.0.conv3.weight\", \"network.layer1.0.bn3.weight\", \"network.layer1.0.bn3.bias\", \"network.layer1.0.bn3.running_mean\", \"network.layer1.0.bn3.running_var\", \"network.layer1.0.downsample.0.weight\", \"network.layer1.0.downsample.1.weight\", \"network.layer1.0.downsample.1.bias\", \"network.layer1.0.downsample.1.running_mean\", \"network.layer1.0.downsample.1.running_var\", \"network.layer1.1.conv1.weight\", \"network.layer1.1.bn1.weight\", \"network.layer1.1.bn1.bias\", \"network.layer1.1.bn1.running_mean\", \"network.layer1.1.bn1.running_var\", \"network.layer1.1.conv2.weight\", \"network.layer1.1.bn2.weight\", \"network.layer1.1.bn2.bias\", \"network.layer1.1.bn2.running_mean\", \"network.layer1.1.bn2.running_var\", \"network.layer1.1.conv3.weight\", \"network.layer1.1.bn3.weight\", \"network.layer1.1.bn3.bias\", \"network.layer1.1.bn3.running_mean\", \"network.layer1.1.bn3.running_var\", \"network.layer1.2.conv1.weight\", \"network.layer1.2.bn1.weight\", \"network.layer1.2.bn1.bias\", \"network.layer1.2.bn1.running_mean\", \"network.layer1.2.bn1.running_var\", \"network.layer1.2.conv2.weight\", \"network.layer1.2.bn2.weight\", \"network.layer1.2.bn2.bias\", \"network.layer1.2.bn2.running_mean\", \"network.layer1.2.bn2.running_var\", \"network.layer1.2.conv3.weight\", \"network.layer1.2.bn3.weight\", \"network.layer1.2.bn3.bias\", \"network.layer1.2.bn3.running_mean\", \"network.layer1.2.bn3.running_var\", \"network.layer2.0.conv1.weight\", \"network.layer2.0.bn1.weight\", \"network.layer2.0.bn1.bias\", \"network.layer2.0.bn1.running_mean\", \"network.layer2.0.bn1.running_var\", \"network.layer2.0.conv2.weight\", \"network.layer2.0.bn2.weight\", \"network.layer2.0.bn2.bias\", \"network.layer2.0.bn2.running_mean\", \"network.layer2.0.bn2.running_var\", \"network.layer2.0.conv3.weight\", \"network.layer2.0.bn3.weight\", \"network.layer2.0.bn3.bias\", \"network.layer2.0.bn3.running_mean\", \"network.layer2.0.bn3.running_var\", \"network.layer2.0.downsample.0.weight\", \"network.layer2.0.downsample.1.weight\", \"network.layer2.0.downsample.1.bias\", \"network.layer2.0.downsample.1.running_mean\", \"network.layer2.0.downsample.1.running_var\", \"network.layer2.1.conv1.weight\", \"network.layer2.1.bn1.weight\", \"network.layer2.1.bn1.bias\", \"network.layer2.1.bn1.running_mean\", \"network.layer2.1.bn1.running_var\", \"network.layer2.1.conv2.weight\", \"network.layer2.1.bn2.weight\", \"network.layer2.1.bn2.bias\", \"network.layer2.1.bn2.running_mean\", \"network.layer2.1.bn2.running_var\", \"network.layer2.1.conv3.weight\", \"network.layer2.1.bn3.weight\", \"network.layer2.1.bn3.bias\", \"network.layer2.1.bn3.running_mean\", \"network.layer2.1.bn3.running_var\", \"network.layer2.2.conv1.weight\", \"network.layer2.2.bn1.weight\", \"network.layer2.2.bn1.bias\", \"network.layer2.2.bn1.running_mean\", \"network.layer2.2.bn1.running_var\", \"network.layer2.2.conv2.weight\", \"network.layer2.2.bn2.weight\", \"network.layer2.2.bn2.bias\", \"network.layer2.2.bn2.running_mean\", \"network.layer2.2.bn2.running_var\", \"network.layer2.2.conv3.weight\", \"network.layer2.2.bn3.weight\", \"network.layer2.2.bn3.bias\", \"network.layer2.2.bn3.running_mean\", \"network.layer2.2.bn3.running_var\", \"network.layer2.3.conv1.weight\", \"network.layer2.3.bn1.weight\", \"network.layer2.3.bn1.bias\", \"network.layer2.3.bn1.running_mean\", \"network.layer2.3.bn1.running_var\", \"network.layer2.3.conv2.weight\", \"network.layer2.3.bn2.weight\", \"network.layer2.3.bn2.bias\", \"network.layer2.3.bn2.running_mean\", \"network.layer2.3.bn2.running_var\", \"network.layer2.3.conv3.weight\", \"network.layer2.3.bn3.weight\", \"network.layer2.3.bn3.bias\", \"network.layer2.3.bn3.running_mean\", \"network.layer2.3.bn3.running_var\", \"network.layer3.0.conv1.weight\", \"network.layer3.0.bn1.weight\", \"network.layer3.0.bn1.bias\", \"network.layer3.0.bn1.running_mean\", \"network.layer3.0.bn1.running_var\", \"network.layer3.0.conv2.weight\", \"network.layer3.0.bn2.weight\", \"network.layer3.0.bn2.bias\", \"network.layer3.0.bn2.running_mean\", \"network.layer3.0.bn2.running_var\", \"network.layer3.0.conv3.weight\", \"network.layer3.0.bn3.weight\", \"network.layer3.0.bn3.bias\", \"network.layer3.0.bn3.running_mean\", \"network.layer3.0.bn3.running_var\", \"network.layer3.0.downsample.0.weight\", \"network.layer3.0.downsample.1.weight\", \"network.layer3.0.downsample.1.bias\", \"network.layer3.0.downsample.1.running_mean\", \"network.layer3.0.downsample.1.running_var\", \"network.layer3.1.conv1.weight\", \"network.layer3.1.bn1.weight\", \"network.layer3.1.bn1.bias\", \"network.layer3.1.bn1.running_mean\", \"network.layer3.1.bn1.running_var\", \"network.layer3.1.conv2.weight\", \"network.layer3.1.bn2.weight\", \"network.layer3.1.bn2.bias\", \"network.layer3.1.bn2.running_mean\", \"network.layer3.1.bn2.running_var\", \"network.layer3.1.conv3.weight\", \"network.layer3.1.bn3.weight\", \"network.layer3.1.bn3.bias\", \"network.layer3.1.bn3.running_mean\", \"network.layer3.1.bn3.running_var\", \"network.layer3.2.conv1.weight\", \"network.layer3.2.bn1.weight\", \"network.layer3.2.bn1.bias\", \"network.layer3.2.bn1.running_mean\", \"network.layer3.2.bn1.running_var\", \"network.layer3.2.conv2.weight\", \"network.layer3.2.bn2.weight\", \"network.layer3.2.bn2.bias\", \"network.layer3.2.bn2.running_mean\", \"network.layer3.2.bn2.running_var\", \"network.layer3.2.conv3.weight\", \"network.layer3.2.bn3.weight\", \"network.layer3.2.bn3.bias\", \"network.layer3.2.bn3.running_mean\", \"network.layer3.2.bn3.running_var\", \"network.layer3.3.conv1.weight\", \"network.layer3.3.bn1.weight\", \"network.layer3.3.bn1.bias\", \"network.layer3.3.bn1.running_mean\", \"network.layer3.3.bn1.running_var\", \"network.layer3.3.conv2.weight\", \"network.layer3.3.bn2.weight\", \"network.layer3.3.bn2.bias\", \"network.layer3.3.bn2.running_mean\", \"network.layer3.3.bn2.running_var\", \"network.layer3.3.conv3.weight\", \"network.layer3.3.bn3.weight\", \"network.layer3.3.bn3.bias\", \"network.layer3.3.bn3.running_mean\", \"network.layer3.3.bn3.running_var\", \"network.layer3.4.conv1.weight\", \"network.layer3.4.bn1.weight\", \"network.layer3.4.bn1.bias\", \"network.layer3.4.bn1.running_mean\", \"network.layer3.4.bn1.running_var\", \"network.layer3.4.conv2.weight\", \"network.layer3.4.bn2.weight\", \"network.layer3.4.bn2.bias\", \"network.layer3.4.bn2.running_mean\", \"network.layer3.4.bn2.running_var\", \"network.layer3.4.conv3.weight\", \"network.layer3.4.bn3.weight\", \"network.layer3.4.bn3.bias\", \"network.layer3.4.bn3.running_mean\", \"network.layer3.4.bn3.running_var\", \"network.layer3.5.conv1.weight\", \"network.layer3.5.bn1.weight\", \"network.layer3.5.bn1.bias\", \"network.layer3.5.bn1.running_mean\", \"network.layer3.5.bn1.running_var\", \"network.layer3.5.conv2.weight\", \"network.layer3.5.bn2.weight\", \"network.layer3.5.bn2.bias\", \"network.layer3.5.bn2.running_mean\", \"network.layer3.5.bn2.running_var\", \"network.layer3.5.conv3.weight\", \"network.layer3.5.bn3.weight\", \"network.layer3.5.bn3.bias\", \"network.layer3.5.bn3.running_mean\", \"network.layer3.5.bn3.running_var\", \"network.layer4.0.conv1.weight\", \"network.layer4.0.bn1.weight\", \"network.layer4.0.bn1.bias\", \"network.layer4.0.bn1.running_mean\", \"network.layer4.0.bn1.running_var\", \"network.layer4.0.conv2.weight\", \"network.layer4.0.bn2.weight\", \"network.layer4.0.bn2.bias\", \"network.layer4.0.bn2.running_mean\", \"network.layer4.0.bn2.running_var\", \"network.layer4.0.conv3.weight\", \"network.layer4.0.bn3.weight\", \"network.layer4.0.bn3.bias\", \"network.layer4.0.bn3.running_mean\", \"network.layer4.0.bn3.running_var\", \"network.layer4.0.downsample.0.weight\", \"network.layer4.0.downsample.1.weight\", \"network.layer4.0.downsample.1.bias\", \"network.layer4.0.downsample.1.running_mean\", \"network.layer4.0.downsample.1.running_var\", \"network.layer4.1.conv1.weight\", \"network.layer4.1.bn1.weight\", \"network.layer4.1.bn1.bias\", \"network.layer4.1.bn1.running_mean\", \"network.layer4.1.bn1.running_var\", \"network.layer4.1.conv2.weight\", \"network.layer4.1.bn2.weight\", \"network.layer4.1.bn2.bias\", \"network.layer4.1.bn2.running_mean\", \"network.layer4.1.bn2.running_var\", \"network.layer4.1.conv3.weight\", \"network.layer4.1.bn3.weight\", \"network.layer4.1.bn3.bias\", \"network.layer4.1.bn3.running_mean\", \"network.layer4.1.bn3.running_var\", \"network.layer4.2.conv1.weight\", \"network.layer4.2.bn1.weight\", \"network.layer4.2.bn1.bias\", \"network.layer4.2.bn1.running_mean\", \"network.layer4.2.bn1.running_var\", \"network.layer4.2.conv2.weight\", \"network.layer4.2.bn2.weight\", \"network.layer4.2.bn2.bias\", \"network.layer4.2.bn2.running_mean\", \"network.layer4.2.bn2.running_var\", \"network.layer4.2.conv3.weight\", \"network.layer4.2.bn3.weight\", \"network.layer4.2.bn3.bias\", \"network.layer4.2.bn3.running_mean\", \"network.layer4.2.bn3.running_var\", \"network.fc.0.weight\", \"network.fc.0.bias\", \"network.fc.1.weight\", \"network.fc.1.bias\", \"network.fc.1.running_mean\", \"network.fc.1.running_var\", \"network.fc.4.weight\", \"network.fc.4.bias\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"model_state_dict\", \"optimizer_state_dict\", \"val_loss\", \"val_acc\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNet(num_class)\n\u001b[0;32m      3\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2629\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2621\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2622\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2623\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2624\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2625\u001b[0m             ),\n\u001b[0;32m   2626\u001b[0m         )\n\u001b[0;32m   2628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2629\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2630\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2631\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2632\u001b[0m         )\n\u001b[0;32m   2633\u001b[0m     )\n\u001b[0;32m   2634\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"network.conv1.weight\", \"network.bn1.weight\", \"network.bn1.bias\", \"network.bn1.running_mean\", \"network.bn1.running_var\", \"network.layer1.0.conv1.weight\", \"network.layer1.0.bn1.weight\", \"network.layer1.0.bn1.bias\", \"network.layer1.0.bn1.running_mean\", \"network.layer1.0.bn1.running_var\", \"network.layer1.0.conv2.weight\", \"network.layer1.0.bn2.weight\", \"network.layer1.0.bn2.bias\", \"network.layer1.0.bn2.running_mean\", \"network.layer1.0.bn2.running_var\", \"network.layer1.0.conv3.weight\", \"network.layer1.0.bn3.weight\", \"network.layer1.0.bn3.bias\", \"network.layer1.0.bn3.running_mean\", \"network.layer1.0.bn3.running_var\", \"network.layer1.0.downsample.0.weight\", \"network.layer1.0.downsample.1.weight\", \"network.layer1.0.downsample.1.bias\", \"network.layer1.0.downsample.1.running_mean\", \"network.layer1.0.downsample.1.running_var\", \"network.layer1.1.conv1.weight\", \"network.layer1.1.bn1.weight\", \"network.layer1.1.bn1.bias\", \"network.layer1.1.bn1.running_mean\", \"network.layer1.1.bn1.running_var\", \"network.layer1.1.conv2.weight\", \"network.layer1.1.bn2.weight\", \"network.layer1.1.bn2.bias\", \"network.layer1.1.bn2.running_mean\", \"network.layer1.1.bn2.running_var\", \"network.layer1.1.conv3.weight\", \"network.layer1.1.bn3.weight\", \"network.layer1.1.bn3.bias\", \"network.layer1.1.bn3.running_mean\", \"network.layer1.1.bn3.running_var\", \"network.layer1.2.conv1.weight\", \"network.layer1.2.bn1.weight\", \"network.layer1.2.bn1.bias\", \"network.layer1.2.bn1.running_mean\", \"network.layer1.2.bn1.running_var\", \"network.layer1.2.conv2.weight\", \"network.layer1.2.bn2.weight\", \"network.layer1.2.bn2.bias\", \"network.layer1.2.bn2.running_mean\", \"network.layer1.2.bn2.running_var\", \"network.layer1.2.conv3.weight\", \"network.layer1.2.bn3.weight\", \"network.layer1.2.bn3.bias\", \"network.layer1.2.bn3.running_mean\", \"network.layer1.2.bn3.running_var\", \"network.layer2.0.conv1.weight\", \"network.layer2.0.bn1.weight\", \"network.layer2.0.bn1.bias\", \"network.layer2.0.bn1.running_mean\", \"network.layer2.0.bn1.running_var\", \"network.layer2.0.conv2.weight\", \"network.layer2.0.bn2.weight\", \"network.layer2.0.bn2.bias\", \"network.layer2.0.bn2.running_mean\", \"network.layer2.0.bn2.running_var\", \"network.layer2.0.conv3.weight\", \"network.layer2.0.bn3.weight\", \"network.layer2.0.bn3.bias\", \"network.layer2.0.bn3.running_mean\", \"network.layer2.0.bn3.running_var\", \"network.layer2.0.downsample.0.weight\", \"network.layer2.0.downsample.1.weight\", \"network.layer2.0.downsample.1.bias\", \"network.layer2.0.downsample.1.running_mean\", \"network.layer2.0.downsample.1.running_var\", \"network.layer2.1.conv1.weight\", \"network.layer2.1.bn1.weight\", \"network.layer2.1.bn1.bias\", \"network.layer2.1.bn1.running_mean\", \"network.layer2.1.bn1.running_var\", \"network.layer2.1.conv2.weight\", \"network.layer2.1.bn2.weight\", \"network.layer2.1.bn2.bias\", \"network.layer2.1.bn2.running_mean\", \"network.layer2.1.bn2.running_var\", \"network.layer2.1.conv3.weight\", \"network.layer2.1.bn3.weight\", \"network.layer2.1.bn3.bias\", \"network.layer2.1.bn3.running_mean\", \"network.layer2.1.bn3.running_var\", \"network.layer2.2.conv1.weight\", \"network.layer2.2.bn1.weight\", \"network.layer2.2.bn1.bias\", \"network.layer2.2.bn1.running_mean\", \"network.layer2.2.bn1.running_var\", \"network.layer2.2.conv2.weight\", \"network.layer2.2.bn2.weight\", \"network.layer2.2.bn2.bias\", \"network.layer2.2.bn2.running_mean\", \"network.layer2.2.bn2.running_var\", \"network.layer2.2.conv3.weight\", \"network.layer2.2.bn3.weight\", \"network.layer2.2.bn3.bias\", \"network.layer2.2.bn3.running_mean\", \"network.layer2.2.bn3.running_var\", \"network.layer2.3.conv1.weight\", \"network.layer2.3.bn1.weight\", \"network.layer2.3.bn1.bias\", \"network.layer2.3.bn1.running_mean\", \"network.layer2.3.bn1.running_var\", \"network.layer2.3.conv2.weight\", \"network.layer2.3.bn2.weight\", \"network.layer2.3.bn2.bias\", \"network.layer2.3.bn2.running_mean\", \"network.layer2.3.bn2.running_var\", \"network.layer2.3.conv3.weight\", \"network.layer2.3.bn3.weight\", \"network.layer2.3.bn3.bias\", \"network.layer2.3.bn3.running_mean\", \"network.layer2.3.bn3.running_var\", \"network.layer3.0.conv1.weight\", \"network.layer3.0.bn1.weight\", \"network.layer3.0.bn1.bias\", \"network.layer3.0.bn1.running_mean\", \"network.layer3.0.bn1.running_var\", \"network.layer3.0.conv2.weight\", \"network.layer3.0.bn2.weight\", \"network.layer3.0.bn2.bias\", \"network.layer3.0.bn2.running_mean\", \"network.layer3.0.bn2.running_var\", \"network.layer3.0.conv3.weight\", \"network.layer3.0.bn3.weight\", \"network.layer3.0.bn3.bias\", \"network.layer3.0.bn3.running_mean\", \"network.layer3.0.bn3.running_var\", \"network.layer3.0.downsample.0.weight\", \"network.layer3.0.downsample.1.weight\", \"network.layer3.0.downsample.1.bias\", \"network.layer3.0.downsample.1.running_mean\", \"network.layer3.0.downsample.1.running_var\", \"network.layer3.1.conv1.weight\", \"network.layer3.1.bn1.weight\", \"network.layer3.1.bn1.bias\", \"network.layer3.1.bn1.running_mean\", \"network.layer3.1.bn1.running_var\", \"network.layer3.1.conv2.weight\", \"network.layer3.1.bn2.weight\", \"network.layer3.1.bn2.bias\", \"network.layer3.1.bn2.running_mean\", \"network.layer3.1.bn2.running_var\", \"network.layer3.1.conv3.weight\", \"network.layer3.1.bn3.weight\", \"network.layer3.1.bn3.bias\", \"network.layer3.1.bn3.running_mean\", \"network.layer3.1.bn3.running_var\", \"network.layer3.2.conv1.weight\", \"network.layer3.2.bn1.weight\", \"network.layer3.2.bn1.bias\", \"network.layer3.2.bn1.running_mean\", \"network.layer3.2.bn1.running_var\", \"network.layer3.2.conv2.weight\", \"network.layer3.2.bn2.weight\", \"network.layer3.2.bn2.bias\", \"network.layer3.2.bn2.running_mean\", \"network.layer3.2.bn2.running_var\", \"network.layer3.2.conv3.weight\", \"network.layer3.2.bn3.weight\", \"network.layer3.2.bn3.bias\", \"network.layer3.2.bn3.running_mean\", \"network.layer3.2.bn3.running_var\", \"network.layer3.3.conv1.weight\", \"network.layer3.3.bn1.weight\", \"network.layer3.3.bn1.bias\", \"network.layer3.3.bn1.running_mean\", \"network.layer3.3.bn1.running_var\", \"network.layer3.3.conv2.weight\", \"network.layer3.3.bn2.weight\", \"network.layer3.3.bn2.bias\", \"network.layer3.3.bn2.running_mean\", \"network.layer3.3.bn2.running_var\", \"network.layer3.3.conv3.weight\", \"network.layer3.3.bn3.weight\", \"network.layer3.3.bn3.bias\", \"network.layer3.3.bn3.running_mean\", \"network.layer3.3.bn3.running_var\", \"network.layer3.4.conv1.weight\", \"network.layer3.4.bn1.weight\", \"network.layer3.4.bn1.bias\", \"network.layer3.4.bn1.running_mean\", \"network.layer3.4.bn1.running_var\", \"network.layer3.4.conv2.weight\", \"network.layer3.4.bn2.weight\", \"network.layer3.4.bn2.bias\", \"network.layer3.4.bn2.running_mean\", \"network.layer3.4.bn2.running_var\", \"network.layer3.4.conv3.weight\", \"network.layer3.4.bn3.weight\", \"network.layer3.4.bn3.bias\", \"network.layer3.4.bn3.running_mean\", \"network.layer3.4.bn3.running_var\", \"network.layer3.5.conv1.weight\", \"network.layer3.5.bn1.weight\", \"network.layer3.5.bn1.bias\", \"network.layer3.5.bn1.running_mean\", \"network.layer3.5.bn1.running_var\", \"network.layer3.5.conv2.weight\", \"network.layer3.5.bn2.weight\", \"network.layer3.5.bn2.bias\", \"network.layer3.5.bn2.running_mean\", \"network.layer3.5.bn2.running_var\", \"network.layer3.5.conv3.weight\", \"network.layer3.5.bn3.weight\", \"network.layer3.5.bn3.bias\", \"network.layer3.5.bn3.running_mean\", \"network.layer3.5.bn3.running_var\", \"network.layer4.0.conv1.weight\", \"network.layer4.0.bn1.weight\", \"network.layer4.0.bn1.bias\", \"network.layer4.0.bn1.running_mean\", \"network.layer4.0.bn1.running_var\", \"network.layer4.0.conv2.weight\", \"network.layer4.0.bn2.weight\", \"network.layer4.0.bn2.bias\", \"network.layer4.0.bn2.running_mean\", \"network.layer4.0.bn2.running_var\", \"network.layer4.0.conv3.weight\", \"network.layer4.0.bn3.weight\", \"network.layer4.0.bn3.bias\", \"network.layer4.0.bn3.running_mean\", \"network.layer4.0.bn3.running_var\", \"network.layer4.0.downsample.0.weight\", \"network.layer4.0.downsample.1.weight\", \"network.layer4.0.downsample.1.bias\", \"network.layer4.0.downsample.1.running_mean\", \"network.layer4.0.downsample.1.running_var\", \"network.layer4.1.conv1.weight\", \"network.layer4.1.bn1.weight\", \"network.layer4.1.bn1.bias\", \"network.layer4.1.bn1.running_mean\", \"network.layer4.1.bn1.running_var\", \"network.layer4.1.conv2.weight\", \"network.layer4.1.bn2.weight\", \"network.layer4.1.bn2.bias\", \"network.layer4.1.bn2.running_mean\", \"network.layer4.1.bn2.running_var\", \"network.layer4.1.conv3.weight\", \"network.layer4.1.bn3.weight\", \"network.layer4.1.bn3.bias\", \"network.layer4.1.bn3.running_mean\", \"network.layer4.1.bn3.running_var\", \"network.layer4.2.conv1.weight\", \"network.layer4.2.bn1.weight\", \"network.layer4.2.bn1.bias\", \"network.layer4.2.bn1.running_mean\", \"network.layer4.2.bn1.running_var\", \"network.layer4.2.conv2.weight\", \"network.layer4.2.bn2.weight\", \"network.layer4.2.bn2.bias\", \"network.layer4.2.bn2.running_mean\", \"network.layer4.2.bn2.running_var\", \"network.layer4.2.conv3.weight\", \"network.layer4.2.bn3.weight\", \"network.layer4.2.bn3.bias\", \"network.layer4.2.bn3.running_mean\", \"network.layer4.2.bn3.running_var\", \"network.fc.0.weight\", \"network.fc.0.bias\", \"network.fc.1.weight\", \"network.fc.1.bias\", \"network.fc.1.running_mean\", \"network.fc.1.running_var\", \"network.fc.4.weight\", \"network.fc.4.bias\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"model_state_dict\", \"optimizer_state_dict\", \"val_loss\", \"val_acc\". "
     ]
    }
   ],
   "source": [
    "num_class = 10\n",
    "model = ResNet(num_class)\n",
    "state_dict = torch.load('best_model.pth', map_location = 'cpu')\n",
    "model.load_state_dict(state_dict , strict = True )\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0c9bb2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "model = ResNet(num_classes = 10)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "116a23d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "opt_func = torch.optim.SGD\n",
    "\n",
    "optimizer = opt_func(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e2140401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, train_loader, val_loader, optimizer, device, checkpoint_path=\"best_model.pth\"):\n",
    "    history = []\n",
    "    best_val_acc = 0.0 \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            train_losses.append(loss)\n",
    "\n",
    "     \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_loss = torch.stack(train_losses).mean().item()\n",
    "        train_acc = train_correct / train_total\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = []\n",
    "            for batch in val_loader:\n",
    "                images, labels = batch\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                out = model.validation_step((images, labels))\n",
    "                val_outputs.append(out)\n",
    "\n",
    "            result = model.validation_epoch_end(val_outputs)\n",
    "            result['train_loss'] = train_loss\n",
    "            result['train_acc'] = train_acc\n",
    "\n",
    "        \n",
    "        val_acc = result['val_acc']\n",
    "        is_best = val_acc > best_val_acc\n",
    "\n",
    "        \n",
    "        model.epoch_end(epoch + 1, result, is_best=is_best)\n",
    "        history.append(result)\n",
    "\n",
    "       \n",
    "        if is_best:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"val_loss\": result['val_loss'],\n",
    "                    \"val_acc\": result['val_acc'],\n",
    "                },\n",
    "                checkpoint_path,\n",
    "            ) \n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e65a7674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Acc: 0.7750 | Train Loss: 0.7549 | Val Loss: 0.2068 | Val Acc: 0.9528  <-- Saving best model \n",
      "Epoch 2 | Train Acc: 0.9599 | Train Loss: 0.1635 | Val Loss: 0.1168 | Val Acc: 0.9685  <-- Saving best model \n",
      "Epoch 3 | Train Acc: 0.9832 | Train Loss: 0.0788 | Val Loss: 0.0768 | Val Acc: 0.9804  <-- Saving best model \n",
      "Epoch 4 | Train Acc: 0.9919 | Train Loss: 0.0443 | Val Loss: 0.0673 | Val Acc: 0.9823  <-- Saving best model \n",
      "Epoch 5 | Train Acc: 0.9968 | Train Loss: 0.0258 | Val Loss: 0.0692 | Val Acc: 0.9787\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "history = fit(epochs, model, train_loader, val_loader, optimizer,\n",
    "              device, checkpoint_path=\"best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28013e5",
   "metadata": {},
   "source": [
    "Saving the model using Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "99c84fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['leaf_disease_model.joblib']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'leaf_disease_model.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8805cd4",
   "metadata": {},
   "source": [
    "Adding Generative AI Compoments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "42eeb27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: torch in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.3.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.20.2)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer-slim->transformers) (8.3.1)\n",
      "Requirement already satisfied: idna in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2025.11.12)\n",
      "Requirement already satisfied: anyio in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f590ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6c68b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3cf4735c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 148/148 [00:01<00:00, 109.24it/s, Materializing param=transformer.wte.weight]             \n",
      "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e296f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline (\n",
    "    \"text-generation\",\n",
    "    model = model_name,\n",
    "    tokenizer = model_name,\n",
    "    torch_dtype = 'auto'\n",
    ")\n",
    "\n",
    "prompt = f\"\"\"\n",
    "### Instruction:\n",
    "You are a professional plant pathologist. A deep learning model has identified a leaf disease in a crop. \n",
    "Provide a concise, expert report for a farmer.\n",
    "\n",
    "### Disease Detected: \n",
    "{predicted_disease}\n",
    "\n",
    "### Report Requirements:\n",
    "1. Symptoms: Briefly describe how to confirm this.\n",
    "2. Immediate Action: What should the farmer do today?\n",
    "3. Organic Treatment: List two natural remedies.\n",
    "4. Prevention: How to avoid this in the next season.\n",
    "\n",
    "### Expert Advice:\n",
    "\"\"\"\n",
    "\n",
    "max_token = 300\n",
    "outputs = generator(prompt, max_new_tokens=max_token)\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50695437",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[43mdevice\u001b[49m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(checkpoint))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "checkpoint = torch.load(\"best_model.pth\", map_location=device)\n",
    "print(type(checkpoint))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
